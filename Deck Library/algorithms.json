[
  {
    "Term": "Sorting Algorithms Overview",
    "Definition": "Sorting algorithms are used to rearrange a list of elements in a particular order (ascending or descending). Sorting is a fundamental operation in computer science, crucial for optimizing the performance of other algorithms.\n\n**Types:**\n- Comparison-based (e.g., quicksort, mergesort)\n- Non-comparison-based (e.g., counting sort, radix sort)\n\n**Importance:** Sorting simplifies problems like searching and improves data organization.\n\n**Benefits:** Provides a foundation for other operations.\n\n**Disadvantages:** Different algorithms have varying complexities and are suited for different data types and sizes.\n\n**Use Cases:** Efficient data organization, database operations, algorithms optimization.",
    "CorrectCount": 0,
    "LastReview": "2024-08-01T00:00:00Z",
    "NextReview": "2024-08-02T00:00:00Z"
  },
  {
    "Term": "Quick Sort",
    "Definition": "Quick sort is an efficient, in-place, comparison-based sorting algorithm. It utilizes the divide-and-conquer approach to sort elements.\n\n**Applications:** Efficient for large datasets, often used in standard libraries for language-specific sort functions.\n\n**Benefits:** Average and best-case time complexity is O(n log n), and it sorts in-place with a small constant factor.\n\n**Disadvantages:** Worst-case time complexity is O(n²), which occurs when the smallest or largest element is always picked as the pivot.\n\n**Pseudocode:**\n```\nfunction quicksort(arr):\n    if len(arr) <= 1:\n        return arr\n    pivot = arr[len(arr) // 2]\n    left = [x for x in arr if x < pivot]\n    middle = [x for x in arr if x == pivot]\n    right = [x for x in arr if x > pivot]\n    return quicksort(left) + middle + quicksort(right)\n```",
    "CorrectCount": 0,
    "LastReview": "2024-08-01T00:00:00Z",
    "NextReview": "2024-08-02T00:00:00Z"
  },
  {
    "Term": "Merge Sort",
    "Definition": "Merge sort is a stable, comparison-based sorting algorithm that divides the input into two halves, recursively sorts them, and then merges the sorted halves.\n\n**Applications:** Useful for sorting linked lists and large datasets that don't fit into memory.\n\n**Benefits:** Consistent O(n log n) time complexity, stable sorting.\n\n**Disadvantages:** Requires additional space proportional to the input size, which can be inefficient for large datasets.\n\n**Pseudocode:**\n```\nfunction mergeSort(arr):\n    if len(arr) > 1:\n        mid = len(arr) // 2\n        leftHalf = arr[:mid]\n        rightHalf = arr[mid:]\n        mergeSort(leftHalf)\n        mergeSort(rightHalf)\n        i = j = k = 0\n        while i < len(leftHalf) and j < len(rightHalf):\n            if leftHalf[i] < rightHalf[j]:\n                arr[k] = leftHalf[i]\n                i += 1\n            else:\n                arr[k] = rightHalf[j]\n                j += 1\n            k += 1\n        while i < len(leftHalf):\n            arr[k] = leftHalf[i]\n            i += 1\n            k += 1\n        while j < len(rightHalf):\n            arr[k] = rightHalf[j]\n            j += 1\n            k += 1\n```",
    "CorrectCount": 0,
    "LastReview": "2024-08-01T00:00:00Z",
    "NextReview": "2024-08-02T00:00:00Z"
  },
  {
    "Term": "Bubble Sort",
    "Definition": "Bubble sort is a simple, comparison-based sorting algorithm that repeatedly steps through the list, compares adjacent elements, and swaps them if they are in the wrong order.\n\n**Applications:** Rarely used in practice due to inefficiency, primarily used for educational purposes.\n\n**Benefits:** Simple to understand and implement.\n\n**Disadvantages:** O(n²) time complexity, inefficient on large lists.\n\n**Pseudocode:**\n```\nfunction bubbleSort(arr):\n    n = len(arr)\n    for i in range(n):\n        swapped = False\n        for j in range(0, n-i-1):\n            if arr[j] > arr[j+1]:\n                arr[j], arr[j+1] = arr[j+1], arr[j]\n                swapped = True\n        if not swapped:\n            break\n```",
    "CorrectCount": 0,
    "LastReview": "2024-08-01T00:00:00Z",
    "NextReview": "2024-08-02T00:00:00Z"
  },
  {
    "Term": "Insertion Sort",
    "Definition": "Insertion sort builds the final sorted array one item at a time by repeatedly picking the next element and inserting it into the already sorted part.\n\n**Applications:** Efficient for small datasets, partially sorted arrays, and linked lists.\n\n**Benefits:** Simple, efficient for small or nearly sorted data.\n\n**Disadvantages:** O(n²) time complexity, inefficient on large unsorted datasets.\n\n**Pseudocode:**\n```\nfunction insertionSort(arr):\n    for i in range(1, len(arr)):\n        key = arr[i]\n        j = i - 1\n        while j >= 0 and key < arr[j]:\n            arr[j + 1] = arr[j]\n            j -= 1\n        arr[j + 1] = key\n```",
    "CorrectCount": 0,
    "LastReview": "2024-08-01T00:00:00Z",
    "NextReview": "2024-08-02T00:00:00Z"
  },
  {
    "Term": "Selection Sort",
    "Definition": "Selection sort is a simple comparison-based sorting algorithm that divides the input list into two parts: the sublist of items already sorted and the sublist of items remaining to be sorted.\n\n**Applications:** Useful for sorting small datasets.\n\n**Benefits:** Simple to implement, performs well on small lists.\n\n**Disadvantages:** O(n²) time complexity, inefficient on large datasets.\n\n**Pseudocode:**\n```\nfunction selectionSort(arr):\n    for i in range(len(arr)):\n        minIndex = i\n        for j in range(i+1, len(arr)):\n            if arr[j] < arr[minIndex]:\n                minIndex = j\n        arr[i], arr[minIndex] = arr[minIndex], arr[i]\n```",
    "CorrectCount": 0,
    "LastReview": "2024-08-01T00:00:00Z",
    "NextReview": "2024-08-02T00:00:00Z"
  },
  {
    "Term": "Heap Sort",
    "Definition": "Heap sort is a comparison-based sorting algorithm that uses a binary heap data structure. It is similar to selection sort where we first find the maximum element and place the maximum element at the end.\n\n**Applications:** Efficient in places where sorting is needed for selecting the largest or smallest element.\n\n**Benefits:** O(n log n) time complexity, in-place sorting.\n\n**Disadvantages:** More complex than simple algorithms like bubble sort or insertion sort.\n\n**Pseudocode:**\n```\nfunction heapSort(arr):\n    buildMaxHeap(arr)\n    for i in range(len(arr)-1, 0, -1):\n        arr[0], arr[i] = arr[i], arr[0]\n        maxHeapify(arr, index=0, size=i)\n```",
    "CorrectCount": 0,
    "LastReview": "2024-08-01T00:00:00Z",
    "NextReview": "2024-08-02T00:00:00Z"
  },
  {
    "Term": "Counting Sort",
    "Definition": "Counting sort is a non-comparison-based sorting algorithm that works by counting the number of occurrences of each element in the input list, then calculates the positions of each element.\n\n**Applications:** Useful for sorting integers when the range of the input is known.\n\n**Benefits:** O(n+k) time complexity where n is the number of elements and k is the range of the input.\n\n**Disadvantages:** Limited to non-negative discrete integers, requires additional space.\n\n**Pseudocode:**\n```\nfunction countingSort(arr):\n    maxValue = max(arr)\n    count = [0] * (maxValue + 1)\n    for num in arr:\n        count[num] += 1\n    index = 0\n    for i, c in enumerate(count):\n        while c > 0:\n            arr[index] = i\n            index += 1\n            c -= 1\n```",
    "CorrectCount": 0,
    "LastReview": "2024-08-01T00:00:00Z",
    "NextReview": "2024-08-02T00:00:00Z"
  },
  {
    "Term": "Radix Sort",
    "Definition": "Radix sort is a non-comparison-based sorting algorithm that sorts integers by processing individual digits, usually starting from the least significant digit to the most significant digit.\n\n**Applications:** Effective for sorting numbers with a fixed number of digits.\n\n**Benefits:** O(nk) time complexity where n is the number of elements and k is the number of digits in the largest number.\n\n**Disadvantages:** Limited to discrete integer inputs, not suitable for negative numbers.\n\n**Pseudocode:**\n```\nfunction radixSort(arr):\n    maxLength = max(len(str(x)) for x in arr)\n    for digit in range(maxLength):\n        buckets = [[] for _ in range(10)]\n        for number in arr:\n            digitValue = (number // (10 ** digit)) % 10\n            buckets[digitValue].append(number)\n        arr = [num for bucket in buckets for num in bucket]\n```",
    "CorrectCount": 0,
    "LastReview": "2024-08-01T00:00:00Z",
    "NextReview": "2024-08-02T00:00:00Z"
  },
  {
    "Term": "Binary Search",
    "Definition": "Binary search is an efficient algorithm for finding an item from a sorted list of items. It works by repeatedly dividing the search interval in half.\n\n**Applications:** Searching in a sorted array or list.\n\n**Benefits:** O(log n) time complexity, efficient for large datasets.\n\n**Disadvantages:** Requires a sorted array, additional overhead for sorting.\n\n**Pseudocode:**\n```\nfunction binarySearch(arr, target):\n    low, high = 0, len(arr) - 1\n    while low <= high:\n        mid = (low + high) // 2\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            low = mid + 1\n        else:\n            high = mid - 1\n    return -1\n```",
    "CorrectCount": 0,
    "LastReview": "2024-08-01T00:00:00Z",
    "NextReview": "2024-08-02T00:00:00Z"
  },
  {
    "Term": "Linear Search",
    "Definition": "Linear search is the simplest searching algorithm that checks every element until the desired element is found or the list ends.\n\n**Applications:** Unsorted or small datasets.\n\n**Benefits:** Simple to implement.\n\n**Disadvantages:** O(n) time complexity, inefficient for large datasets.\n\n**Pseudocode:**\n```\nfunction linearSearch(arr, target):\n    for i, element in enumerate(arr):\n        if element == target:\n            return i\n    return -1\n```",
    "CorrectCount": 0,
    "LastReview": "2024-08-01T00:00:00Z",
    "NextReview": "2024-08-02T00:00:00Z"
  },
  {
    "Term": "Depth First Search (DFS)",
    "Definition": "DFS is an algorithm for traversing or searching tree or graph data structures, starting at the root and exploring as far as possible along each branch before backtracking.\n\n**Applications:** Pathfinding, topological sorting, solving puzzles.\n\n**Benefits:** Simple implementation, uses less memory compared to BFS.\n\n**Disadvantages:** May get trapped in infinite loops, high time complexity for wide or deep trees.\n\n**Pseudocode:**\n```\nfunction DFS(node, visited):\n    if node is not visited:\n        visit(node)\n        visited.add(node)\n        for neighbor in node.neighbors:\n            if neighbor not in visited:\n                DFS(neighbor, visited)\n```",
    "CorrectCount": 0,
    "LastReview": "2024-08-01T00:00:00Z",
    "NextReview": "2024-08-02T00:00:00Z"
  },
  {
    "Term": "Breadth First Search (BFS)",
    "Definition": "BFS is an algorithm for searching or traversing tree or graph data structures, exploring all the neighbors at the present depth before moving on to nodes at the next depth level.\n\n**Applications:** Shortest path finding, level-order traversal of trees.\n\n**Benefits:** Finds shortest path in unweighted graphs.\n\n**Disadvantages:** Uses more memory compared to DFS.\n\n**Pseudocode:**\n```\nfunction BFS(root):\n    queue = [root]\n    visited = set()\n    while queue:\n        node = queue.pop(0)\n        if node not in visited:\n            visit(node)\n            visited.add(node)\n            queue.extend(neighbor for neighbor in node.neighbors if neighbor not in visited)\n```",
    "CorrectCount": 0,
    "LastReview": "2024-08-01T00:00:00Z",
    "NextReview": "2024-08-02T00:00:00Z"
  },
  {
    "Term": "Dijkstra's Algorithm",
    "Definition": "Dijkstra's algorithm is a graph search algorithm that solves the single-source shortest path problem for a graph with non-negative edge weights.\n\n**Applications:** Routing, navigation systems, network analysis.\n\n**Benefits:** Finds shortest paths efficiently in graphs with non-negative weights.\n\n**Disadvantages:** Inefficient for graphs with negative weights or cycles.\n\n**Pseudocode:**\n```\nfunction dijkstra(graph, start):\n    distances = {vertex: float('infinity') for vertex in graph}\n    distances[start] = 0\n    pq = [(0, start)]\n    while pq:\n        current_distance, current_vertex = heapq.heappop(pq)\n        if current_distance > distances[current_vertex]:\n            continue\n        for neighbor, weight in graph[current_vertex]:\n            distance = current_distance + weight\n            if distance < distances[neighbor]:\n                distances[neighbor] = distance\n                heapq.heappush(pq, (distance, neighbor))\n    return distances\n```",
    "CorrectCount": 0,
    "LastReview": "2024-08-01T00:00:00Z",
    "NextReview": "2024-08-02T00:00:00Z"
  },
  {
    "Term": "A* Search Algorithm",
    "Definition": "A* is a search algorithm that finds the shortest path from a start node to a target node, using heuristics to improve performance.\n\n**Applications:** Pathfinding in games and robotics.\n\n**Benefits:** Provides optimal solutions with efficient exploration using heuristics.\n\n**Disadvantages:** Performance heavily depends on the quality of heuristics.\n\n**Pseudocode:**\n```\nfunction a_star(start, goal):\n    openSet = set([start])\n    cameFrom = {}\n    gScore = {start: 0}\n    fScore = {start: heuristic(start, goal)}\n    while openSet:\n        current = node in openSet with lowest fScore[]\n        if current == goal:\n            return reconstruct_path(cameFrom, current)\n        openSet.remove(current)\n        for neighbor in current.neighbors:\n            tentative_gScore = gScore[current] + distance(current, neighbor)\n            if tentative_gScore < gScore.get(neighbor, float('infinity')):\n                cameFrom[neighbor] = current\n                gScore[neighbor] = tentative_gScore\n                fScore[neighbor] = gScore[neighbor] + heuristic(neighbor, goal)\n                if neighbor not in openSet:\n                    openSet.add(neighbor)\n    return False\n```",
    "CorrectCount": 0,
    "LastReview": "2024-08-01T00:00:00Z",
    "NextReview": "2024-08-02T00:00:00Z"
  },
  {
    "Term": "Knapsack Problem",
    "Definition": "The knapsack problem is a combinatorial optimization problem that involves selecting a subset of items, each with a weight and value, to maximize the total value while staying within a weight limit.\n\n**Applications:** Resource allocation, budgeting, cargo loading.\n\n**Benefits:** Illustrates principles of dynamic programming and greedy algorithms.\n\n**Disadvantages:** Computationally expensive, especially for large datasets.\n\n**Pseudocode (0/1 Knapsack):**\n```\nfunction knapsack(weights, values, capacity):\n    n = len(values)\n    K = [[0 for x in range(capacity + 1)] for x in range(n + 1)]\n    for i in range(n + 1):\n        for w in range(capacity + 1):\n            if i == 0 or w == 0:\n                K[i][w] = 0\n            elif weights[i-1] <= w:\n                K[i][w] = max(values[i-1] + K[i-1][w-weights[i-1]], K[i-1][w])\n            else:\n                K[i][w] = K[i-1][w]\n    return K[n][capacity]\n```",
    "CorrectCount": 0,
    "LastReview": "2024-08-01T00:00:00Z",
    "NextReview": "2024-08-02T00:00:00Z"
  },
  {
    "Term": "Floyd-Warshall Algorithm",
    "Definition": "The Floyd-Warshall algorithm is an all-pairs shortest path algorithm that calculates shortest paths between all pairs of vertices in a weighted graph.\n\n**Applications:** Network routing, finding transitive closure.\n\n**Benefits:** Computes shortest paths between all pairs of nodes efficiently.\n\n**Disadvantages:** O(n³) time complexity, inefficient for large graphs.\n\n**Pseudocode:**\n```\nfunction floydWarshall(graph):\n    dist = map(map(row, graph))\n    for k in range(len(graph)):\n        for i in range(len(graph)):\n            for j in range(len(graph)):\n                dist[i][j] = min(dist[i][j], dist[i][k] + dist[k][j])\n    return dist\n```",
    "CorrectCount": 0,
    "LastReview": "2024-08-01T00:00:00Z",
    "NextReview": "2024-08-02T00:00:00Z"
  },
  {
    "Term": "Kruskal's Algorithm",
    "Definition": "Kruskal's algorithm is a minimum spanning tree algorithm that finds the subset of edges that form a tree and connect all vertices in a graph with the minimum possible total edge weight.\n\n**Applications:** Network design, clustering.\n\n**Benefits:** Works well with disconnected graphs, efficiently finds minimum spanning tree.\n\n**Disadvantages:** May require sorting edges, inefficient for dense graphs.\n\n**Pseudocode:**\n```\nfunction kruskal(graph):\n    result = []\n    i, e = 0, 0\n    graph.edges = sorted(graph.edges, key=lambda item: item[2])\n    parent, rank = [], []\n    for node in range(graph.V):\n        parent.append(node)\n        rank.append(0)\n    while e < graph.V - 1:\n        u, v, w = graph.edges[i]\n        i += 1\n        x = find(parent, u)\n        y = find(parent, v)\n        if x != y:\n            e += 1\n            result.append([u, v, w])\n            union(parent, rank, x, y)\n    return result\n```",
    "CorrectCount": 0,
    "LastReview": "2024-08-01T00:00:00Z",
    "NextReview": "2024-08-02T00:00:00Z"
  },
  {
    "Term": "Prim's Algorithm",
    "Definition": "Prim's algorithm is a minimum spanning tree algorithm that finds the subset of edges that form a tree and connect all vertices in a graph with the minimum possible total edge weight.\n\n**Applications:** Network design, clustering.\n\n**Benefits:** Works well with connected graphs, efficiently finds minimum spanning tree.\n\n**Disadvantages:** Requires adjacency matrix, inefficient for dense graphs.\n\n**Pseudocode:**\n```\nfunction prim(graph):\n    key = [float('infinity')] * graph.V\n    parent = [None] * graph.V\n    key[0] = 0\n    mstSet = [False] * graph.V\n    for cout in range(graph.V):\n        u = minKey(key, mstSet)\n        mstSet[u] = True\n        for v in range(graph.V):\n            if graph[u][v] and not mstSet[v] and key[v] > graph[u][v]:\n                key[v] = graph[u][v]\n                parent[v] = u\n    return parent\n```",
    "CorrectCount": 0,
    "LastReview": "2024-08-01T00:00:00Z",
    "NextReview": "2024-08-02T00:00:00Z"
  },
  {
    "Term": "Greedy Algorithms",
    "Definition": "Greedy algorithms make the locally optimal choice at each stage with the hope of finding a global optimum. They are used in optimization problems where the solution involves a sequence of choices.\n\n**Applications:** Prim's and Kruskal's algorithms, Huffman coding, activity selection.\n\n**Benefits:** Simple and efficient for certain problems.\n\n**Disadvantages:** Not always optimal, requires careful problem analysis.\n\n**Example Problems:**\n1. Activity Selection Problem\n2. Fractional Knapsack Problem\n3. Minimum Spanning Tree (Prim's, Kruskal's)\n\n**Example (Activity Selection Problem):**\n```\nfunction activitySelection(start, end, n):\n    selectedActivities = [0]\n    lastEnd = end[0]\n    for i in range(1, n):\n        if start[i] >= lastEnd:\n            selectedActivities.append(i)\n            lastEnd = end[i]\n    return selectedActivities\n```",
    "CorrectCount": 0,
    "LastReview": "2024-08-01T00:00:00Z",
    "NextReview": "2024-08-02T00:00:00Z"
  },
  {
    "Term": "Dynamic Programming",
    "Definition": "Dynamic programming is a method for solving complex problems by breaking them down into simpler subproblems and storing the results of these subproblems to avoid computing the same results multiple times.\n\n**Applications:** Fibonacci sequence, knapsack problem, shortest path problems.\n\n**Benefits:** Efficiently solves overlapping subproblems, reduces time complexity.\n\n**Disadvantages:** Requires additional memory for storing subproblem results.\n\n**Example Problems:**\n1. Fibonacci Sequence\n2. Longest Common Subsequence\n3. 0/1 Knapsack Problem\n\n**Example (Fibonacci Sequence):**\n```\nfunction fibonacci(n):\n    fib = [0, 1]\n    for i in range(2, n+1):\n        fib.append(fib[i-1] + fib[i-2])\n    return fib[n]\n```",
    "CorrectCount": 0,
    "LastReview": "2024-08-01T00:00:00Z",
    "NextReview": "2024-08-02T00:00:00Z"
  },
  {
    "Term": "Divide and Conquer",
    "Definition": "Divide and conquer is an algorithm design paradigm that involves dividing a problem into smaller subproblems, solving them independently, and combining their solutions to solve the original problem.\n\n**Applications:** Merge sort, quicksort, binary search.\n\n**Benefits:** Breaks down complex problems, parallelizable.\n\n**Disadvantages:** Overhead of recursive calls, not always efficient for small subproblems.\n\n**Example Problems:**\n1. Merge Sort\n2. Quick Sort\n3. Binary Search\n\n**Example (Merge Sort):**\n```\nfunction mergeSort(arr):\n    if len(arr) > 1:\n        mid = len(arr) // 2\n        leftHalf = arr[:mid]\n        rightHalf = arr[mid:]\n        mergeSort(leftHalf)\n        mergeSort(rightHalf)\n        i = j = k = 0\n        while i < len(leftHalf) and j < len(rightHalf):\n            if leftHalf[i] < rightHalf[j]:\n                arr[k] = leftHalf[i]\n                i += 1\n            else:\n                arr[k] = rightHalf[j]\n                j += 1\n            k += 1\n        while i < len(leftHalf):\n            arr[k] = leftHalf[i]\n            i += 1\n            k += 1\n        while j < len(rightHalf):\n            arr[k] = rightHalf[j]\n            j += 1\n            k += 1\n```",
    "CorrectCount": 0,
    "LastReview": "2024-08-01T00:00:00Z",
    "NextReview": "2024-08-02T00:00:00Z"
  },
  {
    "Term": "Backtracking",
    "Definition": "Backtracking is a general algorithm for finding solutions to constraint satisfaction problems by incrementally building candidates to the solutions and abandoning a candidate as soon as it is determined that it cannot be completed to a valid solution.\n\n**Applications:** N-Queens, Sudoku, Hamiltonian path.\n\n**Benefits:** Finds all solutions, flexible.\n\n**Disadvantages:** Exponential time complexity, not efficient for large problems.\n\n**Example Problems:**\n1. N-Queens\n2. Sudoku Solver\n3. Hamiltonian Path\n\n**Example (N-Queens):**\n```\nfunction solveNQueens(n):\n    def backtrack(row, diagonals, antiDiagonals, cols):\n        if row == n:\n            solutions.append(board[:])\n            return\n        for col in range(n):\n            currentDiagonal = row - col\n            currentAntiDiagonal = row + col\n            if col in cols or currentDiagonal in diagonals or currentAntiDiagonal in antiDiagonals:\n                continue\n            cols.add(col)\n            diagonals.add(currentDiagonal)\n            antiDiagonals.add(currentAntiDiagonal)\n            board[row] = '.' * col + 'Q' + '.' * (n - col - 1)\n            backtrack(row + 1, diagonals, antiDiagonals, cols)\n            cols.remove(col)\n            diagonals.remove(currentDiagonal)\n            antiDiagonals.remove(currentAntiDiagonal)\n            board[row] = '.' * n\n    solutions = []\n    board = ['.' * n for _ in range(n)]\n    backtrack(0, set(), set(), set())\n    return solutions\n```",
    "CorrectCount": 0,
    "LastReview": "2024-08-01T00:00:00Z",
    "NextReview": "2024-08-02T00:00:00Z"
  },
  {
    "Term": "Fill in the Blank - Quick Sort (Python)",
    "Definition": "Complete the quick sort function:\n```python\n def quicksort(arr):\n     if len(arr) <= 1:\n         return arr\n     pivot = arr[len(arr) // 2]\n     left = [x for x in arr if x < pivot]\n     middle = [x for x in arr if x == pivot]\n     right = [x for x in arr if x > pivot]\n     return quicksort(left) + middle + quicksort(right)\n```",
    "CorrectCount": 0,
    "LastReview": "2024-08-01T00:00:00Z",
    "NextReview": "2024-08-02T00:00:00Z"
  },
  {
    "Term": "Fill in the Blank - Merge Sort (Python)",
    "Definition": "Complete the merge sort function:\n```python\n def merge_sort(arr):\n     if len(arr) > 1:\n         mid = len(arr) // 2\n         left_half = arr[:mid]\n         right_half = arr[mid:]\n         merge_sort(left_half)\n         merge_sort(right_half)\n         i = j = k = 0\n         while i < len(left_half) and j < len(right_half):\n             if left_half[i] < right_half[j]:\n                 arr[k] = left_half[i]\n                 i += 1\n             else:\n                 arr[k] = right_half[j]\n                 j += 1\n             k += 1\n         while i < len(left_half):\n             arr[k] = left_half[i]\n             i += 1\n             k += 1\n         while j < len(right_half):\n             arr[k] = right_half[j]\n             j += 1\n             k += 1\n```",
    "CorrectCount": 0,
    "LastReview": "2024-08-01T00:00:00Z",
    "NextReview": "2024-08-02T00:00:00Z"
  }
]
